{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ee8142",
   "metadata": {},
   "source": [
    "# SentimentScope: Sentiment Analysis using Transformers!\n",
    "## Introduction <a name = \"introduction\"></a>\n",
    "\n",
    "In this notebook, you will train a transformer model from scratch to perform sentiment analysis on the IMDB dataset. You are a Machine Learning Engineer at Cinescope, a growing entertainment company working to enhance its recommendation system. Your task is to fine-tune a transformer-based model for sentiment analysis using the IMDB dataset. By classifying reviews as positive or negative, you will help the company better understand user sentiment and deliver more personalized experiences.\n",
    "\n",
    "By completing this project, you will demonstrate your competency in the following learning objectives:\n",
    "\n",
    "- Load, explore, and prepare a text dataset for training a transformer model using PyTorch.\n",
    "- Customize the architecture of the transformer model for a classification task.\n",
    "- Train and test a transformer model on the IMDB dataset.\n",
    "\n",
    "Now that you have an overview of what you will achieve in this project, let’s move on to the project outline.\n",
    "\n",
    "---\n",
    "\n",
    "### Project Outline\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "\n",
    "1. [Introduction](#introduction): Overview of the project, learning objectives, and understanding sentiment analysis.\n",
    "2. [Load, Explore, and Prepare the Dataset](#load-explore-and-prepare-the-dataset): Load the IMDB dataset, explore it with visualizations, and split it into training and validation sets.\n",
    "3. [Implement a DataLoader in PyTorch](#implement-a-dataloader-in-pytorch): Create the `IMDBDataset` class and use it with the PyTorch `DataLoader`, including tokenization.\n",
    "4. [Customize the Transformer Architecture](#customize-the-transformer-architecture): Modify the transformer model for binary classification.\n",
    "5. [Implement Accuracy Calculation Method](#implement-accuracy-calculation-method): Create a function to compute accuracy for monitoring performance.\n",
    "6. [Train the Model](#train-the-model): Complete and execute the training loop for binary classification.\n",
    "7. [Test the Model](#test-the-model): Evaluate the model on the test dataset and ensure it achieves over 75% accuracy.\n",
    "8. [Conclusion](#conclusion): Summarize the project results and key takeaways.\n",
    "\n",
    "Click on the section titles above to navigate directly to the corresponding part of the notebook!\n",
    "\n",
    "---\n",
    "\n",
    "Now that we've outlined the structure and objectives of this project, let's delve into the core concept: sentiment analysis.\n",
    "\n",
    "### Understanding Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is a natural language processing (NLP) technique used to determine the sentiment expressed in a piece of text. This can range from identifying the polarity (positive, negative, or neutral) of a review to analyzing emotions and opinions.\n",
    "\n",
    "In this project, sentiment analysis is explicitly framed as a **binary classification task**, where the goal is to determine whether a given movie review is *positive* or *negative*. This task is central to many real-world applications, including customer feedback analysis, social media monitoring, and recommendation systems. By developing a transformer-based model, you will classify IMDB movie reviews as positive or negative to tackle the challenge faced by your entertainment company CineScope by enhancing its recommendation system, enabling more accurate and personalized suggestions. \n",
    "\n",
    "Reviews labeled as positive will be marked as 1 in the dataset, while negative reviews will be labeled as 0.\n",
    "\n",
    "For example, consider the following movie review:\n",
    "\n",
    "> \"The movie was a rollercoaster of emotions, and I loved every moment of it!\"\n",
    "\n",
    "This review is clearly positive as it expresses enjoyment and satisfaction with the movie, hence it will be labelled as *positive* or 1 in the dataset. In contrast:\n",
    "\n",
    "> \"The plot was predictable, and the acting was subpar. A waste of time.\"\n",
    "\n",
    "This review conveys a negative sentiment, criticizing both the plot and acting, hence it will be labelled as *negative* or 0 in the dataset.\n",
    "\n",
    "While transformers are often used for generation tasks, they can also be adapted for classification tasks with some modifications to their architecture. You might already be familiar with the tweaks that we will implement in this project.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Data Description\n",
    "\n",
    "The dataset used in this project is the [IMDB dataset](https://ai.stanford.edu/~amaas/data/sentiment/), provided in the `aclIMDB_v1.tar.gz` file. Upon extracting the file, you will find the following folder structure:\n",
    "\n",
    "```\n",
    "aclIMDB/\n",
    "├── train/\n",
    "│   ├── pos/    # Positive reviews for training\n",
    "│   ├── neg/    # Negative reviews for training\n",
    "│   ├── unsup/  # Unsupervised data (not used in this project)\n",
    "├── test/\n",
    "│   ├── pos/    # Positive reviews for testing\n",
    "│   ├── neg/    # Negative reviews for testing\n",
    "```\n",
    "\n",
    "- **train/**: Contains labeled data for training the model. Reviews in the `pos/` folder should be labeled as positive (1), while reviews in the `neg/` folder should be labeled as negative (0).\n",
    "- **test/**: Contains labeled data for evaluating the model. Similar to the training data, `pos/` and `neg/` contain positive and negative reviews, respectively.\n",
    "- **unsup/**: Contains unlabeled reviews that are not used in this project.\n",
    "\n",
    "Understanding the folder structure is crucial as it guides how we load and preprocess the data for the sentiment classification task.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738efb7",
   "metadata": {},
   "source": [
    "## <a name=\"load-explore-and-prepare-the-dataset\"></a>Load, Explore, and Prepare the Dataset\n",
    "\n",
    "### 1. Load the Dataset\n",
    "The dataset is already available in the environment as `aclIMDB_v1.tar.gz`. We will load it into Pandas DataFrames for easy exploration and preparation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c2264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d4e8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the dataset - uncomment the line below to run\n",
    "!tar -xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b411455",
   "metadata": {},
   "source": [
    "You have successfully extracted the folder. Go back to your workspace and explore the folder structure and find the relative paths for each of the following:\n",
    "- Training positive reviews\n",
    "- Training negative reviews\n",
    "- Testing positive reviews\n",
    "- Testing negative reviews\n",
    "\n",
    "Assign the paths of these folders relative to the starter file in the variables below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a4caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to dataset\n",
    "train_pos_path = 'aclImdb/train/pos' # Path to the directory containing positive reviews from the training set\n",
    "train_neg_path = 'aclImdb/train/neg' # Path to the directory containing negative reviews from the training set\n",
    "test_pos_path = 'aclImdb/test/pos' # Path to the directory containing positive reviews from the test set\n",
    "test_neg_path = 'aclImdb/test/neg' # Path to the directory containing negative reviews from the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02785d",
   "metadata": {},
   "source": [
    "Now, you will implement the `load_dataset()` function, which reads all text files in a specified folder and returns their content as a list of strings. This function is essential for loading and preprocessing the dataset in subsequent steps.\n",
    "\n",
    "To implement this function:\n",
    "\n",
    "1. **Use the `os` module**: Leverage Python's `os` module to list all files in the folder.\n",
    "2. **Handle file paths**: Use `os.path.join()` to construct full paths for files, ensuring compatibility across operating systems.\n",
    "3. **Read file content**: Open each file in read mode (`'r'`) using UTF-8 encoding to handle text properly.\n",
    "4. **Aggregate results**: Append the content of each file to a list and return it.\n",
    "\n",
    "### Key Points to Consider:\n",
    "- Ensure that the function only processes text files (you may use file extensions for filtering if needed).\n",
    "- Refer to the [os.listdir documentation](https://docs.python.org/3/library/os.html#os.listdir) for listing files in a directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88450b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_dataset(folder):\n",
    "    \"\"\"\n",
    "    Reads all text files in the specified folder and returns their content as a list.\n",
    "\n",
    "    Args:\n",
    "        folder (str): Path to the folder containing text files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings, where each string is the content of a text file.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    # 1. Use the os module to list all files in the folder\n",
    "    for filename in os.listdir(folder):\n",
    "        # Key Point: Ensure the function only processes text files\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # 2. Handle file paths: Use os.path.join to construct the full path\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            \n",
    "            # 3. Read file content: Open each file in read mode ('r') using UTF-8 encoding\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "            # 4. Aggregate results: Append the content to the list\n",
    "            reviews.append(content)\n",
    "            \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a2d6c",
   "metadata": {},
   "source": [
    "Use the function now to load the training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc1e6397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "train_pos = load_dataset(train_pos_path)\n",
    "train_neg = load_dataset(train_neg_path)\n",
    "test_pos = load_dataset(test_pos_path)\n",
    "test_neg = load_dataset(test_neg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c84cd",
   "metadata": {},
   "source": [
    "We can convert the data into pandas dataframes to make handling the datasets easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e3891dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  label\n",
      "0  Silly, hilarious, tragic, sad, inevitable.<br ...      1\n",
      "1  I actually like the original, and this film ha...      1\n",
      "2  For my humanities quarter project for school, ...      1\n",
      "3  To me this was more a wake up call, and realiz...      1\n",
      "4  This movie is a lot of fun. What makes it grea...      1\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames\n",
    "train_df = pd.DataFrame({\n",
    "    'review': train_pos + train_neg,\n",
    "    'label': [1] * len(train_pos) + [0] * len(train_neg)\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'review': test_pos + test_neg,\n",
    "    'label': [1] * len(test_pos) + [0] * len(test_neg)\n",
    "})\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549af2ab",
   "metadata": {},
   "source": [
    "You can ensure that your datasets have loaded correctly by running the following code cell. No output means success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d8ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that both datasets have the expected number of rows\n",
    "assert train_df.shape[0] == 25000, \"Training dataset does not have 25000 rows.\"\n",
    "assert test_df.shape[0] == 25000, \"Testing dataset does not have 25000 rows.\"\n",
    "\n",
    "# Assert that both datasets have exactly two columns\n",
    "assert train_df.shape[1] == 2, \"Training dataset does not have exactly 2 columns.\"\n",
    "assert test_df.shape[1] == 2, \"Testing dataset does not have exactly 2 columns.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d436403",
   "metadata": {},
   "source": [
    "### 2. Explore the Dataset\n",
    "Exploration helps us understand the dataset's structure and distribution.\n",
    "\n",
    "Here are some suggestions for exploration and visualizations:\n",
    "- **Dataset Overview**: Use `DataFrame.info()` and `DataFrame.describe()` to understand the dataset structure and basic statistics.\n",
    "- **Label Distribution**: Create bar charts to visualize the number of positive and negative reviews.\n",
    "- **Review Length Analysis**: Compute and plot the distribution of review lengths (e.g., number of characters or words).\n",
    "- **Sample Reviews**: Print a few positive and negative reviews to understand the text content.\n",
    "\n",
    "Write code to explore the dataset in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea33097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  25000 non-null  object\n",
      " 1   label   25000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  25000 non-null  object\n",
      " 1   label   25000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  25000.00000\n",
       "mean       0.50000\n",
       "std        0.50001\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.50000\n",
       "75%        1.00000\n",
       "max        1.00000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.describe()\n",
    "\n",
    "test_df.info()\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d39d7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution for Training Data\n",
    "train_counts = train_df['label'].value_counts()\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(train_counts.index, train_counts.values)\n",
    "plt.title('Train Dataset Label Distribution')\n",
    "plt.xlabel('Review Type (0=Negative, 1=Positive)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Distribution for Test Data\n",
    "test_counts = test_df['label'].value_counts()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(test_counts.index, test_counts.values)\n",
    "plt.title('Test Dataset Label Distribution')\n",
    "plt.xlabel('Review Type (0=Negative, 1=Positive)')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4885fb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate length of reviews (number of words)\n",
    "train_df['review_length'] = train_df['review'].apply(lambda x: len(x.split()))\n",
    "# Assuming 'review_length' column is already created\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(train_df['review_length'], bins=50)\n",
    "plt.title('Distribution of Review Lengths (Words)')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a1e0b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Positive Reviews ---\n",
      "[\"I've seen this movie, when I was traveling in Brazil. I found it difficult to really understand Brazilian culture and society, because it has so many regional and class differences. To see this movie in Sao Paulo itself was a revelation. It shows something of the everyday life of many Brazilians. On the other side, it is sometimes a little bit over-dramatized. And that's the only negative comment I have on this film. It's sometimes too much, too much sex, too many murders and too much cynicism for one film. The director could film some things a bit more subtle, it would make the film more effective.<br /><br />Despite this I liked the movie and the way the story unravels itself. The characters are complex, and very much like real-life people. Not pretty American actors and actresses with a lot of cosmetics, but people who could be ugly and beautiful at the same time. That makes the film realistic, even when the story is not that convincing.\", \"yeah, it's a bit of a silly film, so if you are looking for an oscar performance, forget this one......but, if you love John Candy's humor, this is a must-see. We lost John Candy before he made enough of his great brand of comedy, and he is only better in one movie: Planes, Trains, & Automobiles (with Steve Martin). Excellent supporting performance by Eugene Levy, perhaps his best work ever as the hot-headed Sal DiPasquale. Also good acting by Richard Libertini, Alley Mills & Pat Hingle. You must see this obscure and out-of-print film if you are a John Candy or Eugene Levy fan.\", \"I had the pleasure of viewing this beautiful film last night, with the wonderful addition of a question and answer session with the director following the viewing. I suspect that the first commenter has never lost a parent or someone very close to them in death. I have had many such losses, and this movie spoke to me. One of the major themes is how we don't deal with questions/issues/stories with our loved ones until it's too late--they're too incapacitated or dead before that happens. Talk to your loved ones, listen to and record their stories, tell people you love them, resolve differences. I loved the message that there are no mistakes. I love the director's portrayal of the relationship of the two daughters--as one of six siblings, it's clear to me he understood how complex those relationships are. His history as a cinematographer also comes through loud and clear--what a beautiful movie! The casting is outstanding--a film not to be missed!\"]\n",
      "\n",
      "--- Sample Negative Reviews ---\n",
      "[\"dont ever ever ever consider watching this sorry excuse for a film. the way it is shot, lit, acted etc. just doesn't make sense. it's all so bad it is difficult to watch. loads of clips are repeated beyond boredom. there seems to be no 'normal' person in the entire film and the existence of the 'outside world' is, well, it just doesn't exist. and why does that bald guy become invincible all of a sudden? this film is beyond stupidity. zero.\", 'CREEPSHOW 2 is the ill-fated sequel to the George Romero\\'s (overrated) original, CREEPSHOW. Any sequel following a Romero film that\\'s not directed by Romero himself has got some large shoes to fill, mostly because of the Romero fans out there who think he\\'s God. I didn\\'t care much for the first film and funny enough, I didn\\'t care much for the sequel. The film series had so much potential but it was short-lived because both films were less than stellar. <br /><br />The biggest problem with CREEPSHOW 2 was that it only had three stories (excluding the in-between story), and because the first story sucked beyond belief, it only left the chance for almost half of the movie to be *really* good. I saw CS 2 at the movies and the first segment was a real groaner. Anything dramatic with George \\'I can\\'t act\\' Kennedy is automatically doomed and the Indian Statue story was too hokey and simply didn\\'t belong in this sequel. So after a really trite and dull start, there were only two other stories left to reverse the fiasco of the aborted beginning and unfortunately the two other stories weren\\'t great enough for me to forget the first story. THE RAFT and THE HITCHHIKER are moderately successful, moderately because though the two other segments have their moments, they still sorta fall flat. The two last stories are basically stretched out for too long. It\\'s not that I wanted the stories to happen at a dizzying pace and end fast, but both good ideas found within those stories were sorta nullified by the fact that they were slow and padded and eventually fell flat when the segments needed to be more energized, more erratic and with punchier endings. Also, if the two last stories hadn\\'t been stretched out to pad the movie or had all three segments been more brief with better editing and direction, they could have added a much needed fourth story to the bunch. Having only three padded segments made for a boring feast.<br /><br />The acting and writing in both THE RAFT and THE HITCHHIKER segments are from awful to good. I like Lois Chiles in the last segment. It\\'s probably her best moment on screen aside from her role as Bond Girl Holly Goodhead in MOONRAKER and in DEATH ON THE NILE. But even her role is difficult to understand at times because of the serviceable direction and the unfocused story. Are we supposed to hate her or sympathize with her? Are we supposed to sympathize with the annoying hitchhiker? If the hitchhiker\\'s body was found by other people on the road, what was he when he attacked Chiles? Was he a ghost or a zombie or what? How did the body eventually left the presence of the other people who found him dead in order to attack Chiles? The whole thing is not very clear, even for a supernatural story. And the ending is rather dull and uneventful.<br /><br />As for the infamous THE RAFT story, well, the acting is mostly on the awful side and none of the characters are sympathetic or interesting. The characters would have been more interesting had the actors played themselves. None of the actors are convincing in their specific roles. Paul Satterfield looks smarter than the dumb jock he\\'s playing and the actress who plays his girlfriend is not very convincing as the typical bitchy slut. She seems too timid. The same could be said with the two others who play the \"plain\" teens. The idea of the killer oil slick is interesting and creepy but not well executed. There should have been a fifth character to the story, maybe a homeless man or a ranger who lurks around the lake and knows about the oil slick and could have been the watery monster\\'s alter ego of sorts. As creepy as the oil slick is, it doesn\\'t make for a compelling \"character\". And the way the story ends, everything seems pointless. No punch to it whatsoever.<br /><br />Except for the few titillating aspects which always seems to make boring things worthwhile, seeing CREEPSHOW 2 at a theater was basically a waste of money and time. CS 2 is more rental material than something you pay to see on the big screen.', 'Neat premise. Very unrealistic. What I learned from this movie is that speeding crazily out of control to go to the weekend cabin may not be the best idea after all. I loved how Bill conveniently rolls out of the car and down the hill with no injury at all! Unfortunately, the same can\\'t be said for his gal. Oh, and the police never seemed to find the car or trace the owner of the wreck.<br /><br />Lots of dragged out scenes including a plain stripper (still have nightmares from that scene). Poor assistant guy and his crummy useless hand. I admit I was intrigued to see what the mysterious \"thing\" was behind the door, but when it appeared, I just laughed. HA HA HA!! The girl really seemed sadistically angry about being revived. Personally, I really would want a new body after an excruciating experience like that!']\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Sample Positive Reviews ---\")\n",
    "print(train_df[train_df['label'] == 1]['review'].sample(3).tolist())\n",
    "print(\"\\n--- Sample Negative Reviews ---\")\n",
    "print(train_df[train_df['label'] == 0]['review'].sample(3).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d70008",
   "metadata": {},
   "source": [
    "### 3. Prepare the Dataset\n",
    "We will split the training data further into training and validation subsets. The way we constructed the dataset, reviews with positive and negative labels are segregated. To ensure that the validation dataset works well, we first need to shuffle the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e696ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data into training and validation sets manually\n",
    "train_size = int(0.9 * len(train_df))\n",
    "# Shuffle the dataset\n",
    "shuffled_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_data = shuffled_df.iloc[:train_size]\n",
    "val_data = shuffled_df.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5c258",
   "metadata": {},
   "source": [
    "### 4. Testing the Tokenizer\n",
    "\n",
    "#### Subword Tokenization\n",
    "In earlier tasks, you might have encountered character-level tokenization, where each character in the text is treated as a token. While this is straightforward, it is less efficient and may result in larger input sizes, impacting the performance of transformer models.\n",
    "\n",
    "To address this, we will use Hugging Face's `AutoTokenizer`, a robust and efficient class designed for tokenizing text based on pretrained models. Specifically, we will utilize the `bert-base-uncased` tokenizer, which applies **subword tokenization**. Subword tokenization involves two steps:\n",
    "\n",
    "1. **Subword Splitting**: Words are split into smaller components (subwords) based on a predefined vocabulary. For example:\n",
    "   - Input: \"unhappiness\"\n",
    "   - Subword Splits: `['un', 'happiness']`\n",
    "\n",
    "2. **Token Conversion**: Each subword is then converted into a numerical token ID. For example:\n",
    "   - Subword Splits: `['un', 'happiness']`\n",
    "   - Token IDs: `[1011, 24123]` (values are illustrative and depend on the tokenizer vocabulary).\n",
    "\n",
    "#### About the `bert-base-uncased` Tokenizer\n",
    "The `bert-base-uncased` tokenizer, developed by Google researchers, is part of the BERT model family. This tokenizer is associated with the `bert-base-uncased` model, which has been widely used for tasks such as sentiment analysis, question answering, and text classification. The tokenizer ensures all text is converted to lowercase and accents are removed, reducing vocabulary size and improving generalization. Example:\n",
    "\n",
    "- Input: \"I Love Transformers.\"\n",
    "- Subword Splits: `['i', 'love', 'trans', '##formers', '.']`\n",
    "\n",
    "You can learn more about the tokenizer and model on [Hugging Face's bert-base-uncased page](https://huggingface.co/bert-base-uncased).\n",
    "\n",
    "#### Using the `AutoTokenizer` Class\n",
    "The `AutoTokenizer` class in the Hugging Face Transformers library provides a seamless way to load tokenizers for various pretrained models. It automatically selects the correct tokenizer configuration based on the model name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10071ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d1f02",
   "metadata": {},
   "source": [
    "This initializes a tokenizer tailored for the `bert-base-uncased` model. You can refer to the [Hugging Face AutoTokenizer documentation](https://huggingface.co/docs/transformers/main_classes/tokenizer) for more details. You can find the definition of the `from_pretrained()` method [here](https://huggingface.co/docs/transformers/v4.48.0/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained).\n",
    "\n",
    "Instead of tokenizing the entire dataset, we will test the tokenizer on a few sample reviews directly from the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5d749b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take sample inputs from the dataset\n",
    "sample_texts = train_data['review'].sample(3, random_state=42).tolist()\n",
    "\n",
    "# Tokenize sample inputs\n",
    "tokenized_samples = tokenizer(sample_texts, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18a88f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996, 10828,  2006,  2029,  2023,  3185,  2003,  2241,  3464,\n",
      "          2028,  1997,  1996,  2087,  2540,  1011, 14916,  4667,  2808,  1045,\n",
      "          2031,  2412,  3191,  1012,  2009,  4136,  1996,  6429,  3441,  1997,\n",
      "          2048,  5208,  1010,  2119,  2040,  3687, 13347,  1998,  4847,  2551,\n",
      "          2092,  2046,  2037,  3963,  1005,  1055,  2004,  1037,  3836,  1998,\n",
      "          1037, 24385,  1010,  2059,  2973,  2178,  2382,  2086,  2007, 13372,\n",
      "          1012, 10090,  9266, 15539,  1996,  2143,  2007,  2014,  6669, 16371,\n",
      "          6651,  2094,  2836,  2004,  1996, 22614,  1000,  2304,  2121,  1000,\n",
      "         29298,  1010,  1996, 24385,  1012,  2016,  2025,  2069, 16783,  2014,\n",
      "          4963,  1010, 17076,  3367,  1010,  1998,  9866,  2092,  1025,  2016,\n",
      "         11082,  2017,  2113,  3599,  2073,  2027,  1005,  2310,  2272,  2013,\n",
      "          2478,  2019,  4610,  1997,  2616,  1012, 22939,  7295,  2078, 10767,\n",
      "          2038,  1996,  2514,  1997,  1996,  3080,  2905,   102],\n",
      "        [  101,  1998,  4298,  7541,  2000,  1996, 19675,  2466,  2240,  1012,\n",
      "          2348,  1045,  2424,  1996,  2402,  1041, 10609, 25733,  2524,  2000,\n",
      "          3422,  1006,  2040,  1005,  1055,  2801,  2001,  2008,  2558,  2606,\n",
      "          1010,  7543,  2027,  2071,  2031,  2589,  2488,  2084,  2008,   999,\n",
      "          1007,  1010,  3660,  2515,  2019,  9788,  3105,  2004,  8040,  3217,\n",
      "         23884,  1012,  2010,  6959,  1997,  2070,  1997,  1996,  3210,  2013,\n",
      "         19675,  2633,  2716,  2009,  2000,  2166,  2005,  2033,  1012,  3487,\n",
      "         19133,  2003,  2673,  2057,  5987,  1998,  2062,  1997,  1996,  5745,\n",
      "          1997,  4234,  2556,  1012,  1045,  2424,  1043,  1012,  1039,  1012,\n",
      "          3660,  1005,  1055,  8040,  3217, 23884,  2172,  2062,  1997,  1037,\n",
      "         19337,  2666, 12423, 28616,  2121,  2084,  1996,  2062,  2783,  2544,\n",
      "          2589,  2011,  4754,  5954,  1012,  1996,  3496,  4234,  2851,  2043,\n",
      "          8040,  3217, 23884, 10919,  2002,  8440,  1005,   102],\n",
      "        [  101,  2040,  2052,  2031,  2245,  2008,  2107,  2019, 14485,  2210,\n",
      "          2143,  2071,  2022,  2061, 20161,  1998,  7244,  1029,  1045,  2572,\n",
      "          2428,  7622,  1012,  2009,  1005,  1055,  1037,  9467,  2008,  2062,\n",
      "          2111,  2031,  2025,  2464,  2009,  1012,  1045,  3866,  1010,  2004,\n",
      "          2467,  1010,  7003,  1062,  5714,  5017,  1005,  1055,  3556,  1012,\n",
      "          1998,  2054,  1037, 21635,  2834,  2011,  6795,  3123,   999,  2664,\n",
      "          1045,  4687,  2065,  1045,  2323,  2655,  2023,  1037,  5469,  2143,\n",
      "          1012,  2009,  2071,  4089,  2022,  5275,  2008,  2009,  2003,  1037,\n",
      "          5913,  2030,  1037,  3689,  2004,  2092,  1012,  2092,  1010,  7539,\n",
      "          1010,  1045,  2293,  1996, 17841,  3512,  4022,  2009,  2038,  1012,\n",
      "          2673,  1998,  3071,  1999,  4698,  1005,  1055,  1006,  2209,  2011,\n",
      "          5904,  9894,  1007,  5544,  5836,  1037,  2613,  4736,  1999,  2014,\n",
      "          2166,  1012,  1012,  1012,  1996,  2160,  2993,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a2d16",
   "metadata": {},
   "source": [
    "Explanation of parameters:\n",
    " - `truncation=True`: Truncates text longer than the specified max_length.\n",
    " - `padding=True`: Pads shorter sequences to match max_length.\n",
    " - `max_length=128`: Specifies the maximum length of the sequences.\n",
    " - `return_tensors=\"pt\"`: Returns PyTorch tensors as the output format.\n",
    " \n",
    "For more details about truncation and padding, refer to the [Hugging Face Padding and Truncation Documentation](https://huggingface.co/docs/transformers/pad_truncation). This step ensures that the tokenizer works as expected and provides insight into its behavior. Next, we will use the tokenizer within the class definition to process the dataset.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ca44c",
   "metadata": {},
   "source": [
    "# Implement a DataLoader in PyTorch<a id=\"implement-a-dataloader-in-pytorch\"></a>\n",
    "\n",
    "\n",
    "In this section, you will implement a custom dataset class and use it to create a DataLoader in PyTorch for feeding data into the model during training. PyTorch simplifies this process by providing the `Dataset` and `DataLoader` classes, which handle batching, shuffling, and preprocessing, allowing you to focus on the model architecture and training.\n",
    "\n",
    "To start, we will create a custom dataset class for the IMDB dataset, which will process and return tokenized inputs along with their corresponding labels. This class will use a tokenizer to preprocess the raw text data.\n",
    "\n",
    "### 1. Define a Custom Dataset Class\n",
    "\n",
    "The custom dataset class will inherit from `torch.utils.data.Dataset` and include the following features:\n",
    "\n",
    "1. **Initialization (`__init__`)**:\n",
    "   - Accepts raw text and label data, along with a tokenizer and a maximum sequence length.\n",
    "   - The tokenizer is used to preprocess the text data into tokenized inputs.\n",
    "   - The maximum sequence length ensures that all tokenized inputs are of uniform length.\n",
    "\n",
    "2. **Length (`__len__`)**:\n",
    "   - Returns the total number of data samples in the dataset.\n",
    "\n",
    "3. **Item Retrieval (`__getitem__`)**:\n",
    "   - Retrieves a single data point by index.\n",
    "   - Preprocesses the text using the tokenizer to create tokenized input IDs.\n",
    "   - Returns the tokenized input IDs and the corresponding label for the given index.\n",
    "\n",
    "You can refer to [this](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) tutorial on the Pytorch website for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a9bb55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4da99b",
   "metadata": {},
   "source": [
    "**NOTE ABOUT GPU USAGE**\n",
    "\n",
    "The workspace provides you with access to a GPU which is necessary for training a transformer model due to the efficiency provided by GPUs on the large amount of computations that are required. To ensure judicious usage of limited resources, please usage the GPU only when you are training the model. \n",
    "\n",
    "---\n",
    "We will keep the maximum length of input to 128 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6976b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset for the IMDB dataset.\n",
    "\n",
    "    This class preprocesses text data using a tokenizer and returns tokenized inputs\n",
    "    along with their corresponding labels for sentiment analysis.\n",
    "\n",
    "    Attributes:\n",
    "        data (pd.DataFrame): A DataFrame containing text and label columns.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for preprocessing text.\n",
    "        max_length (int): Maximum length for tokenized sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=MAX_LENGTH):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): A DataFrame with columns `review` (text) and `label` (target).\n",
    "            tokenizer (transformers.PreTrainedTokenizer): The tokenizer to preprocess the text.\n",
    "            max_length (int, optional): Maximum token sequence length. Defaults to 128.\n",
    "        \"\"\"\n",
    "        #TODO: Write code to initialize the dataset\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples.\n",
    "        \"\"\"\n",
    "        #TODO: Write code to return the length of the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single data point by index and preprocess it.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data point to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tokenized input IDs for the text.\n",
    "            int: Label corresponding to the text.\n",
    "        \"\"\"\n",
    "        review = str(self.data.iloc[idx]['review'])\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,        # Add [CLS] and [SEP]\n",
    "            max_length=self.max_length,     # Truncate/Pad to max_length\n",
    "            padding='max_length',           # Pad to max_length\n",
    "            truncation=True,                # Truncate if longer than max_length\n",
    "            return_attention_mask=True,     # Return attention mask\n",
    "            return_tensors='pt',            # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # The tokenizer returns tensors in a dictionary format.\n",
    "        # We need to squeeze them to remove the unnecessary batch dimension (dimension 0)\n",
    "        # return encoding['input_ids'].squeeze(0), label\n",
    "        # The features dictionary\n",
    "        features = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # For training/evaluation, return the dictionary and the label\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc972e9",
   "metadata": {},
   "source": [
    "### 2. Initialize the Dataset\n",
    "\n",
    "Once the `IMDBDataset` class is defined, we can initialize it directly with the training and validation DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a44edbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the datasets\n",
    "train_dataset = IMDBDataset(train_data, tokenizer)\n",
    "val_dataset = IMDBDataset(val_data, tokenizer)\n",
    "test_dataset = IMDBDataset(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29f274",
   "metadata": {},
   "source": [
    "### 3. Create a DataLoader\n",
    "\n",
    "The `DataLoader` class in PyTorch helps manage batches of data during training. We will use it to create training and validation data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bc21e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600f797",
   "metadata": {},
   "source": [
    "\n",
    "You may have seen the use of `RandomShuffler` along with `DataLoader`. That approach is used when you want to train a model over a dataset randomly for a specified number of steps. In this model, we want to use the epoch approach.\n",
    "\n",
    "An **epoch** is one complete pass through the entire dataset. When using the `DataLoader` in this setup, it ensures that every data point in the dataset is used exactly once during a single epoch. This approach is helpful for training models in a structured manner, ensuring that the model sees all the training examples and learns from them in each epoch before moving to the next one.\n",
    "\n",
    "By setting `shuffle=True` for the `train_loader`, the data points are randomly shuffled at the start of each epoch, improving the generalization of the model. You will see this in action later in the code for training the model.\n",
    "\n",
    "---\n",
    "Given below are some assert statements to check your custom dataset and data loader definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eac2f3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: {'input_ids': tensor([ 101, 1045, 2371, 1037, 2307, 6569, 1010, 2044, 3773, 2023, 2143, 1010,\n",
      "        2025, 2138, 2009, 2003, 1037, 3040, 3538, 1010, 2021, 2138, 2009, 6427,\n",
      "        2033, 1997, 1010, 2008, 1996, 5077, 5988, 2150, 2428, 2200, 2204, 1012,\n",
      "        2057, 2064, 2156, 2182, 1996, 2190, 5077, 3364, 2229, 1999, 2023, 2492,\n",
      "        1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "label shape: 1\n"
     ]
    }
   ],
   "source": [
    "assert len(train_dataset) == 22500, \"Train dataset length mismatch!\"\n",
    "assert len(val_dataset) == 2500, \"Validation dataset length mismatch!\"\n",
    "assert len(test_dataset) == 25000, \"Test dataset length mismatch!\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Check the first item in the train dataset\n",
    "input_ids, label = train_dataset[0]\n",
    "print(f\"Input IDs shape: {input_ids}\")\n",
    "print(f\"label shape: {label}\")\n",
    "input_id = input_ids['input_ids']\n",
    "assert isinstance(input_id, torch.Tensor), \"Input IDs should be a torch.Tensor!\"\n",
    "assert isinstance(label, (int, np.integer)), \"Label should be an integer or int-like!\"\n",
    "\n",
    "# Ensure the input IDs tensor has the correct shape\n",
    "assert input_id.shape[0] == train_dataset.max_length, \"Input IDs tensor has incorrect length!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b7cad",
   "metadata": {},
   "source": [
    "## <a id=\"customize-the-transformer-architecture\"></a>Customize the Transformer Architecture\n",
    "\n",
    "In this section, you will customize the transformer architecture to suit the task of binary classification. You may have used a similar architecture in the past for generation tasks. But you will need to make a few tweaks specifically in the `DemoGPT` class to adapt it for the binary classification.\n",
    "\n",
    "### 1. Config Dictionary\n",
    "Your config dictionary bundles all hyperparameters and model settings in one place. Below is the config that we will use in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "839a72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocabulary_size\": tokenizer.vocab_size,  # e.g., ~30522 for bert-base-uncased\n",
    "    \"num_classes\": 2,                         # binary classification (pos/neg)\n",
    "    \"d_embed\": 128,\n",
    "    \"context_size\": MAX_LENGTH,\n",
    "    \"layers_num\": 4,\n",
    "    \"heads_num\": 4,\n",
    "    \"head_size\": 32,  # 4 heads * 32 = 128 -> matches d_embed\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"use_bias\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ea474",
   "metadata": {},
   "source": [
    "Key Config Parameters:\n",
    "- `vocabulary_size`: The total number of tokens in your vocabulary.\n",
    "- `num_classes`: The number of classes for the classification head (2 = binary).\n",
    "- `d_embed`: Dimensionality of embeddings (and hidden layers).\n",
    "- `context_size`: Maximum sequence length for each input.\n",
    "- `layers_num`: Number of stacked transformer blocks.\n",
    "- `heads_num`: Number of attention heads in multi-head attention.\n",
    "- `head_size`: Dimension of each attention head (must satisfy heads_num * head_size = d_embed).\n",
    "- `dropout_rate`: Probability of dropping units during training to reduce overfitting.\n",
    "- `use_bias`: Whether linear layers should have bias terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e497450a",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Class Definitions\n",
    "\n",
    "Below are the class definitions you will work with. These classes form the core components of the transformer model. You may have seen these before, with the exception of the `DemoGPT` class which will need to be customized.\n",
    "\n",
    "\n",
    "#### AttentionHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "77c51747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Q_weights = nn.Linear(config[\"d_embed\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
    "        self.K_weights = nn.Linear(config[\"d_embed\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
    "        self.V_weights = nn.Linear(config[\"d_embed\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "        casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "        self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, tokens_num, d_embed = input.shape\n",
    "        Q = self.Q_weights(input)  # (B, T, head_size)\n",
    "        K = self.K_weights(input)  # (B, T, head_size)\n",
    "        V = self.V_weights(input)  # (B, T, head_size)\n",
    "\n",
    "        # Q @ K^T => (B, T, T)\n",
    "        attention_scores = Q @ K.transpose(1, 2)\n",
    "\n",
    "        # Casual Mask\n",
    "        attention_scores = attention_scores.masked_fill(\n",
    "            self.casual_attention_mask[:tokens_num, :tokens_num] == 0,\n",
    "            float('-inf')\n",
    "        )\n",
    "        attention_scores = attention_scores / math.sqrt(K.shape[-1])\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "        return attention_scores @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a901a4d",
   "metadata": {},
   "source": [
    "Here we use a dummy input aligned with our config:\n",
    "\n",
    "- Batch size = `BATCH_SIZE` (32)\n",
    "- Sequence length = `config[\"context_size\"]` (128)\n",
    "- Embedding dimension = `config[\"d_embed\"]` (128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "15dd4a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionHead output shape: torch.Size([32, 128, 32])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the AttentionHead\n",
    "attention_head = AttentionHead(config).to(device)\n",
    "\n",
    "# Create a dummy input of shape (32, 128, 128)\n",
    "dummy_input = torch.randn(BATCH_SIZE, config[\"context_size\"], config[\"d_embed\"]).to(device)\n",
    "\n",
    "# Forward pass\n",
    "attention_output = attention_head(dummy_input)\n",
    "print(\"AttentionHead output shape:\", attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b5fb2",
   "metadata": {},
   "source": [
    "Expected shape:\n",
    "\n",
    ">`(B,T,head_size)=(32,128,32)`\n",
    "\n",
    "#### MultiHeadAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b3411a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "        self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "        self.linear = nn.Linear(config[\"heads_num\"] * config[\"head_size\"], config[\"d_embed\"])\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, input):\n",
    "        heads_outputs = [head(input) for head in self.heads]\n",
    "        x = torch.cat(heads_outputs, dim=-1)  # (B, T, heads_num * head_size)\n",
    "        x = self.linear(x)                   # (B, T, d_embed)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8acb2f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention output shape: torch.Size([32, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate MultiHeadAttention\n",
    "multi_head_attention = MultiHeadAttention(config).to(device)\n",
    "\n",
    "# Same dummy input: (32, 128, 128)\n",
    "dummy_input = torch.randn(BATCH_SIZE, config[\"context_size\"], config[\"d_embed\"]).to(device)\n",
    "\n",
    "# Forward pass\n",
    "mha_output = multi_head_attention(dummy_input)\n",
    "print(\"MultiHeadAttention output shape:\", mha_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f945a8",
   "metadata": {},
   "source": [
    "Expected shape:\n",
    "\n",
    ">`(B,T,d_embed)=(32,128,128)`\n",
    "\n",
    "#### FeedForward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d5617365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(config[\"d_embed\"], 4 * config[\"d_embed\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config[\"d_embed\"], config[\"d_embed\"]),\n",
    "            nn.Dropout(config[\"dropout_rate\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f8071288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward output shape: torch.Size([32, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate FeedForward\n",
    "feed_forward = FeedForward(config).to(device)\n",
    "\n",
    "# Dummy input: (32, 128, 128)\n",
    "dummy_input = torch.randn(BATCH_SIZE, config[\"context_size\"], config[\"d_embed\"]).to(device)\n",
    "\n",
    "# Forward pass\n",
    "ff_output = feed_forward(dummy_input)\n",
    "print(\"FeedForward output shape:\", ff_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd023d0",
   "metadata": {},
   "source": [
    "Expected shape:\n",
    "\n",
    ">`(B,T,d_embed)=(32,128,128)`\n",
    "\n",
    "#### Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "19fac2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head = MultiHeadAttention(config)\n",
    "        self.layer_norm_1 = nn.LayerNorm(config[\"d_embed\"])\n",
    "\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config[\"d_embed\"])\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = x + self.multi_head(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ba526a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block output shape: torch.Size([32, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a single Block\n",
    "block = Block(config).to(device)\n",
    "\n",
    "# Dummy input: (32, 128, 128)\n",
    "dummy_input = torch.randn(BATCH_SIZE, config[\"context_size\"], config[\"d_embed\"]).to(device)\n",
    "\n",
    "# Forward pass\n",
    "block_output = block(dummy_input)\n",
    "print(\"Block output shape:\", block_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76658b88",
   "metadata": {},
   "source": [
    "Expected shape:\n",
    "\n",
    ">`(B,T,d_embed)=(32,128,128)`\n",
    "\n",
    "#### DemoGPT\n",
    "Below is the starter code for the `DemoGPT` class, which implements the core of a transformer model tailored for a binary classification task. This implementation builds on the foundation of a transformer architecture and includes the necessary modifications to adapt it for classification. \n",
    "\n",
    "### Key Changes for Binary Classification\n",
    "\n",
    "To adapt the transformer for classification, the following changes are required:\n",
    "\n",
    "1. **Add a Classification-Specific Output Layer**:\n",
    "   - The model needs a linear layer to map the final pooled embeddings to the number of classes. \n",
    "   - The classification head is implemented using [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) with:\n",
    "     - `in_features` set to `d_embed` (the embedding dimension).\n",
    "     - `out_features` set to `num_classes` (the number of classes, 2 for binary classification).\n",
    "     - `bias` set to `False` (optional; bias can be excluded to slightly simplify computations).\n",
    "\n",
    "\n",
    "2. **Implement a Pooling Mechanism**:\n",
    "\n",
    "   - Transformers output embeddings for each token in the input sequence. For classification, these token-level embeddings need to be condensed into a single vector.\n",
    "\n",
    "   - A **mean pooling operation** is applied using [`torch.mean`](https://pytorch.org/docs/stable/generated/torch.mean.html) across the time dimension (`dim=1`) to aggregate token-level embeddings into a single representation vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "80a79d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the DemoGPT class with configuration parameters.\n",
    "\n",
    "        Args:\n",
    "        - config (dict): Configuration dictionary with the following keys:\n",
    "            - \"vocabulary_size\": Size of the vocabulary.\n",
    "            - \"d_embed\": Dimensionality of the embedding vectors.\n",
    "            - \"context_size\": Maximum sequence length (context size).\n",
    "            - \"layers_num\": Number of transformer layers.\n",
    "            - \"num_classes\": Number of output classes (2 for binary classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Token embedding layer: Maps token indices to embedding vectors.\n",
    "        self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"d_embed\"])\n",
    "        \n",
    "        # Positional embedding layer: Adds positional information to the embeddings.\n",
    "        self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"d_embed\"])\n",
    "        \n",
    "        # Transformer layers: Stacked sequence of transformer blocks.\n",
    "        blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "        self.layers = nn.Sequential(*blocks)\n",
    "        \n",
    "        # Layer normalization: Applied to stabilize training.\n",
    "        self.layer_norm = nn.LayerNorm(config[\"d_embed\"])\n",
    "        \n",
    "        # TODO: Implement classification output layer - Maps pooled embeddings to class logits.\n",
    "        self.classification_head = nn.Linear(\n",
    "            in_features=config['d_embed'],\n",
    "            out_features=config['num_classes'],\n",
    "            bias=False  # Optional, as suggested for simplification\n",
    "        )\n",
    "\n",
    "    def forward(self, token_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "        - token_ids (torch.Tensor): Input token indices of shape (B, T), \n",
    "                                    where B is the batch size, and T is the sequence length.\n",
    "        \n",
    "        Returns:\n",
    "        - logits (torch.Tensor): Output logits of shape (B, num_classes).\n",
    "        \"\"\"\n",
    "        batch_size, tokens_num = token_ids.shape\n",
    "\n",
    "        # Step 1: Create embeddings for tokens and their positions\n",
    "        x = self.token_embedding_layer(token_ids)  # Shape: (B, T, d_embed)\n",
    "        #positions = torch.arange(tokens_num, device=token_ids.device)  # Shape: (T,)\n",
    "        positions = torch.arange(token_ids.size(1), device=token_ids.device) # Shape: (T,)\n",
    "        pos_embed = self.positional_embedding_layer(positions)  # Shape: (T, d_embed)\n",
    "        # Add positional embeddings to token embeddings\n",
    "        x = x + pos_embed.unsqueeze(0) # Shape: (B, T, d_embed) + (1, T, d_embed)\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Step 2: Pass embeddings through transformer layers\n",
    "        x = self.layers(x)  # Shape: (B, T, d_embed)\n",
    "        x = self.layer_norm(x)  # Normalize across the feature dimension\n",
    "        \n",
    "        # Step 3: TODO: Apply mean pooling across the time dimension  # Shape: (B, d_embed)\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Step 4: TODO: Generate logits for classification  # Shape: (B, num_classes)\n",
    "        logits = self.classification_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "55f250b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DemoGPT output shape: torch.Size([32, 2])\n",
      "Logits sample:\n",
      " tensor([[-0.0898, -0.0323],\n",
      "        [ 0.0100, -0.1469]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "demo_gpt = DemoGPT(config).to(device)\n",
    "\n",
    "# Suppose we have a batch of size 32, each with a sequence length of 128\n",
    "dummy_token_ids = torch.randint(\n",
    "    0, config[\"vocabulary_size\"], \n",
    "    (BATCH_SIZE, config[\"context_size\"])\n",
    ").to(device)\n",
    "\n",
    "# Forward pass\n",
    "logits = demo_gpt(dummy_token_ids)\n",
    "\n",
    "print(\"DemoGPT output shape:\", logits.shape)\n",
    "print(\"Logits sample:\\n\", logits[:2])  # Print first two examples' logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecea5a4",
   "metadata": {},
   "source": [
    "Expected shape:\n",
    "\n",
    "> `(B,num_classes)=(32,2)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ce0aa703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the number of logits matches the number of classes\n",
    "assert logits.size(1) == config[\"num_classes\"], (\n",
    "    f\"Expected number of classes {config['num_classes']}, \"\n",
    "    f\"but got {logits.size(1)}\"\n",
    ")\n",
    "\n",
    "# Assert that the batch size of the output matches the input batch size\n",
    "assert logits.size(0) == BATCH_SIZE, (\n",
    "    f\"Expected batch size {BATCH_SIZE}, \"\n",
    "    f\"but got {logits.size(0)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ef046",
   "metadata": {},
   "source": [
    "## Implement Accuracy Calculation Method <a name=\"implement-accuracy-calculation-method\"></a>\n",
    "\n",
    "In this section, you will learn how to calculate the validation accuracy for the transformer model on the IMDB dataset. Validation accuracy provides a performance metric that helps assess how well the model generalizes to unseen data during training.\n",
    "\n",
    "### 1. Overview\n",
    "The function to calculate validation accuracy will:\n",
    "\n",
    "- Evaluate the model on the validation dataset.\n",
    "- Generate predictions for each batch.\n",
    "- Compare predictions with the true labels.\n",
    "- Compute the percentage of correctly classified examples.\n",
    "\n",
    "### 2.  Key Points\n",
    "- **Evaluation Mode**: Calling `model.eval()` ensures that dropout and other training-specific layers are disabled during evaluation.\n",
    "- **No Gradients**: The `torch.no_grad()` context disables gradient computation, reducing memory usage and speeding up validation.\n",
    "- **Predictions**: `torch.argmax(logits, dim=1)` retrieves the index of the highest logit for each sample, which corresponds to the predicted class label.\n",
    "- **Accuracy Calculation**: The function computes the fraction of correct predictions out of the total number of samples, then multiplies by 100 to express it as a percentage.\n",
    "\n",
    "After calculating the validation accuracy, incorporate this function into your training loop. Typically, you would call calculate_accuracy at the end of each epoch or after a specific number of training steps. Monitoring validation accuracy over time helps you track performance gains and identify potential overfitting or underfitting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "df0701b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the model on the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained transformer model.\n",
    "        data_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "        device (torch.device): Device to run the model (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        float: Validation accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    # 1. Key Point: Evaluation Mode\n",
    "    model.eval()  # Disables dropout and batch normalization tracking\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # 2. Key Point: No Gradients\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Assuming the DataLoader yields (features_dict, labels_tensor)\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Move labels to the specified device\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Unpack the input features and move to device\n",
    "            # print(f\"Input IDs shape: {inputs}\")\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "            # Evaluate the model\n",
    "            # Assuming the model's forward method accepts input_ids and attention_mask\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # 3. Key Point: Predictions (argmax)\n",
    "            # Find the predicted class index (0 or 1)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # 4. Accuracy Calculation: Compare predictions with the true labels\n",
    "            # Count where predictions match the true labels\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            # Count the total number of samples in the current batch\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "    # Compute the final accuracy\n",
    "    # Key Point: Multiply by 100 to express it as a percentage\n",
    "    accuracy = (total_correct / total_samples) * 100 \n",
    "    \n",
    "    # Reset model to training mode (important if you intend to continue training)\n",
    "    model.train() \n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e92ac596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoGPT(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ec013daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Validation Accuracy: 49.88%\n"
     ]
    }
   ],
   "source": [
    "validation_accuracy = calculate_accuracy(model, val_loader, device)\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df3efa2",
   "metadata": {},
   "source": [
    "As you can see, the validation accuracy is close to 50%. This is expected as the model's parameters have been initialized randomly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd68435",
   "metadata": {},
   "source": [
    "## Train the Model <a name=\"train-the-model\"></a>\n",
    "\n",
    "In this section, we will define the training loop for the transformer-based model designed for sentiment analysis. The training loop is crucial for optimizing the model's weights and biases to minimize the loss function and improve classification performance.\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "The training loop will involve the following steps:\n",
    "\n",
    "1. **Iterate through epochs**: Repeat the training process for a predefined number of epochs.\n",
    "2. **Load batches of data**: Use the `DataLoader` to retrieve batches of input IDs and labels.\n",
    "3. **Forward pass**: Compute the logits by passing the input IDs through the model.\n",
    "4. **Compute loss**: Use cross-entropy loss as the criterion.\n",
    "5. **Backward pass and optimization**: Backpropagate the loss and update the model parameters using the optimizer.\n",
    "6. **Validation**: Calculate the validation accuracy after each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "93b5a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/704], Loss: 0.6940\n",
      "Epoch [1/10], Step [200/704], Loss: 0.6809\n",
      "Epoch [1/10], Step [300/704], Loss: 0.6485\n",
      "Epoch [1/10], Step [400/704], Loss: 0.6233\n",
      "Epoch [1/10], Step [500/704], Loss: 0.6065\n",
      "Epoch [1/10], Step [600/704], Loss: 0.5739\n",
      "Epoch [1/10], Step [700/704], Loss: 0.5621\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Epoch 1 - Validation Accuracy: 66.96%\n",
      "Epoch [2/10], Step [100/704], Loss: 0.5195\n",
      "Epoch [2/10], Step [200/704], Loss: 0.5025\n",
      "Epoch [2/10], Step [300/704], Loss: 0.5186\n",
      "Epoch [2/10], Step [400/704], Loss: 0.5041\n",
      "Epoch [2/10], Step [500/704], Loss: 0.5106\n",
      "Epoch [2/10], Step [600/704], Loss: 0.4968\n",
      "Epoch [2/10], Step [700/704], Loss: 0.4885\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Epoch 2 - Validation Accuracy: 74.76%\n",
      "Epoch [3/10], Step [100/704], Loss: 0.4335\n",
      "Epoch [3/10], Step [200/704], Loss: 0.4487\n",
      "Epoch [3/10], Step [300/704], Loss: 0.4392\n",
      "Epoch [3/10], Step [400/704], Loss: 0.4459\n",
      "Epoch [3/10], Step [500/704], Loss: 0.4326\n",
      "Epoch [3/10], Step [600/704], Loss: 0.4367\n",
      "Epoch [3/10], Step [700/704], Loss: 0.4210\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Epoch 3 - Validation Accuracy: 77.40%\n",
      "Epoch [4/10], Step [100/704], Loss: 0.3876\n",
      "Epoch [4/10], Step [200/704], Loss: 0.4015\n",
      "Epoch [4/10], Step [300/704], Loss: 0.3930\n",
      "Epoch [4/10], Step [400/704], Loss: 0.3771\n",
      "Epoch [4/10], Step [500/704], Loss: 0.3903\n",
      "Epoch [4/10], Step [600/704], Loss: 0.3587\n",
      "Epoch [4/10], Step [700/704], Loss: 0.3765\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Epoch 4 - Validation Accuracy: 78.28%\n",
      "Epoch [5/10], Step [100/704], Loss: 0.3333\n",
      "Epoch [5/10], Step [200/704], Loss: 0.3193\n",
      "Epoch [5/10], Step [300/704], Loss: 0.3197\n",
      "Epoch [5/10], Step [400/704], Loss: 0.3326\n",
      "Epoch [5/10], Step [500/704], Loss: 0.3221\n",
      "Epoch [5/10], Step [600/704], Loss: 0.3438\n",
      "Epoch [5/10], Step [700/704], Loss: 0.3417\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Epoch 5 - Validation Accuracy: 76.68%\n",
      "Epoch [6/10], Step [100/704], Loss: 0.2705\n",
      "Epoch [6/10], Step [200/704], Loss: 0.2789\n",
      "Epoch [6/10], Step [300/704], Loss: 0.2828\n",
      "Epoch [6/10], Step [400/704], Loss: 0.2700\n",
      "Epoch [6/10], Step [500/704], Loss: 0.2870\n",
      "Epoch [6/10], Step [600/704], Loss: 0.3066\n",
      "Epoch [6/10], Step [700/704], Loss: 0.2903\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Epoch 6 - Validation Accuracy: 79.16%\n",
      "Epoch [7/10], Step [100/704], Loss: 0.2392\n",
      "Epoch [7/10], Step [200/704], Loss: 0.2395\n",
      "Epoch [7/10], Step [300/704], Loss: 0.2373\n",
      "Epoch [7/10], Step [400/704], Loss: 0.2233\n",
      "Epoch [7/10], Step [500/704], Loss: 0.2562\n",
      "Epoch [7/10], Step [600/704], Loss: 0.2319\n",
      "Epoch [7/10], Step [700/704], Loss: 0.2546\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Epoch 7 - Validation Accuracy: 79.72%\n",
      "Epoch [8/10], Step [100/704], Loss: 0.1811\n",
      "Epoch [8/10], Step [200/704], Loss: 0.1888\n",
      "Epoch [8/10], Step [300/704], Loss: 0.2030\n",
      "Epoch [8/10], Step [400/704], Loss: 0.2087\n",
      "Epoch [8/10], Step [500/704], Loss: 0.1993\n",
      "Epoch [8/10], Step [600/704], Loss: 0.1984\n",
      "Epoch [8/10], Step [700/704], Loss: 0.2015\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Epoch 8 - Validation Accuracy: 79.44%\n",
      "Epoch [9/10], Step [100/704], Loss: 0.1630\n",
      "Epoch [9/10], Step [200/704], Loss: 0.1572\n",
      "Epoch [9/10], Step [300/704], Loss: 0.1543\n",
      "Epoch [9/10], Step [400/704], Loss: 0.1509\n",
      "Epoch [9/10], Step [500/704], Loss: 0.1567\n",
      "Epoch [9/10], Step [600/704], Loss: 0.1728\n",
      "Epoch [9/10], Step [700/704], Loss: 0.1672\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Epoch 9 - Validation Accuracy: 79.88%\n",
      "Epoch [10/10], Step [100/704], Loss: 0.1178\n",
      "Epoch [10/10], Step [200/704], Loss: 0.1110\n",
      "Epoch [10/10], Step [300/704], Loss: 0.1185\n",
      "Epoch [10/10], Step [400/704], Loss: 0.1100\n",
      "Epoch [10/10], Step [500/704], Loss: 0.1312\n",
      "Epoch [10/10], Step [600/704], Loss: 0.1315\n",
      "Epoch [10/10], Step [700/704], Loss: 0.1496\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Epoch 10 - Validation Accuracy: 79.32%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 10\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DemoGPT(config).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        input_ids = input_ids['input_ids'].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 1. TODO: Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. TODO: Implement forward pass\n",
    "        # Pass input IDs and attention mask through the model to get logits\n",
    "        attention_mask = None\n",
    "        logits = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # 3. TODO: Calculate loss\n",
    "        # Use cross-entropy loss (criterion) to compare logits with true labels\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # 4. TODO: Backward pass\n",
    "        # Compute gradients of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. TODO: Step the optimizer\n",
    "        # Update model parameters based on the computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log training progress\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{step+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {running_loss/100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Evaluate validation accuracy\n",
    "    val_accuracy = calculate_accuracy(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1} - Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af98266a",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- The `evaluate_accuracy` function calculates the model's accuracy on the validation dataset. Ensure this function is defined and works as expected.\n",
    "- The training progress is logged every 100 steps to monitor performance.\n",
    "- After each epoch, the validation accuracy is printed to ensure the model generalizes well to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d4fd56",
   "metadata": {},
   "source": [
    "## Test the Model <a name=\"test-the-model\"></a>\n",
    "\n",
    "In this section, you will evaluate the performance of your trained transformer model on the test dataset. Testing the model involves loading the test dataset, passing it through the model, and calculating the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7d0167a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([32, 128])\n",
      "Input IDs shape: torch.Size([8, 128])\n",
      "Test Accuracy: 76.82%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate the accuracy of the model over the test set using the calculate_accuracy() function\n",
    "\n",
    "test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6f2a4",
   "metadata": {},
   "source": [
    "\n",
    "With the accuracy calculated, you can verify if the model meets the project goal of achieving greater than 75% accuracy on the test dataset. \n",
    "\n",
    "Try training your model for more than 3 epochs! Try increasing the size of the embedding used in the model. Or try increasing the number of blocks or layers in the model. You may be able to improve the accuracy of your model further!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb72f72",
   "metadata": {},
   "source": [
    "## Conclusion <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ed75d",
   "metadata": {},
   "source": [
    "1. Project Results Summary  \n",
    " Objective and Methodology:  \n",
    " The goal was to build and train a custom transformer-based model (DemoGPT) for binary sentiment classification on the IMDB dataset, \n",
    " essential for enhancing Cinescope's recommendation system.  \n",
    " The process involved defining a custom PyTorch Dataset and DataLoader for tokenization and batching.  \n",
    " The model architecture was adapted by applying mean pooling across the sequence dimension and including a final linear classification head.   \n",
    " The model was trained for 10 epochs using the AdamW optimizer.  \n",
    " Final Performance:  \n",
    " Training demonstrated strong learning capabilities, with the final epoch yielding a Validation Accuracy of 79.32%.   \n",
    " The model was then evaluated on the dedicated test dataset, achieving a final Test Accuracy of 76.82%.  \n",
    " Goal Achievement:  \n",
    " The model successfully met the project goal of achieving greater than $75\\%$ accuracy on the test dataset,   \n",
    " validating the efficacy of the custom transformer architecture for this classification task.  \n",
    "2. List key takeaways  \n",
    "Takeaway 1:  \n",
    "Transformer Adaptation for Classification: A primary learning was the necessity of adapting a sequence-to-sequence transformer (like the core architecture of DemoGPT) for a sequence-to-single-label classification task. This was critically achieved by implementing a mean pooling operation (x.mean(dim=1)) to consolidate token-level sequence embeddings $\\mathbf{(B, T, d_{embed})}$ into a single fixed-size vector $\\mathbf{(B, d_{embed})}$ before passing it to the final classifier.  \n",
    "Takeaway 2:  \n",
    "Robust Custom Data Pipeline: The project highlighted the importance of a robust custom data pipeline. Implementing the IMDBDataset with .iloc indexing solved slicing issues and, combined with AutoTokenizer, ensured correct handling of subword tokenization, padding, and the necessary return of a features dictionary (for input_ids and attention_mask) to properly feed the PyTorch DataLoader and the transformer model.  \n",
    "Takeaway 3:  \n",
    "Effective Use of Hyperparameters: Observing the performance gain from $49.88\\%$ initial accuracy to over $79\\%$ validation accuracy demonstrated the direct impact of the chosen AdamW optimizer (with a learning rate of $3e-4$) and the extended training period of $\\mathbf{10\\ epochs}$ on model convergence and generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
